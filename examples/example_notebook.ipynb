{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trismik Adaptive Testing Showcase\n",
    "\n",
    "This notebook demonstrates how to use the Trismik SDK for adaptive testing of language models. Trismik is a platform offering adversarial testing for LLMs that allows you to evaluate models up to 95% faster than traditional evaluation techniques. Our adaptive testing algorithm estimates model precision by examining only a small portion of a dataset, providing efficient evaluation across multiple dimensions like reasoning, toxicity, and tool use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our API keys. You'll need both a Trismik API key and an OpenAI API key for the full examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Uncomment and set these if you want to override the .env file\n",
    "# os.environ['TRISMIK_API_KEY'] = 'your-trismik-api-key-here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-api-key-here'\n",
    "\n",
    "# Configure your project and experiment identifiers\n",
    "# These are required for all Trismik runs and help organize your results\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID\" # Replace with your project ID\n",
    "EXPERIMENT = \"default\"         # Replace with your experiment name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the necessary modules and check our available tests:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import asyncio\n",
    "from typing import Any\n",
    "\n",
    "from trismik.adaptive_test import AdaptiveTest\n",
    "from trismik.types import (\n",
    "    AdaptiveTestScore,\n",
    "    TrismikItem,\n",
    "    TrismikMultipleChoiceTextItem,\n",
    "    TrismikRunMetadata,\n",
    ")\n",
    "\n",
    "# Initialize a test runner to list available datasets\n",
    "runner = AdaptiveTest(lambda x: x)  # Dummy processor for listing datasets\n",
    "\n",
    "# List available datasets\n",
    "available_datasets = runner.list_datasets()\n",
    "print(\"Available datasets:\")\n",
    "for dataset in available_datasets:\n",
    "    print(f\"- {dataset.id}\")\n",
    "\n",
    "dataset = available_datasets[0].id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Mock Inference Example\n",
    "\n",
    "Let's start with a simple mock inference function that demonstrates how the adaptive testing framework works. This example shows the basic structure of how to process test items and run adaptive tests."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def mock_inference(item: TrismikItem) -> Any:\n",
    "    \"\"\"\n",
    "    Process a test item and return a response.\n",
    "    \n",
    "    This is where you would call your model to run inference and return its\n",
    "    response. For demonstration purposes, we simply pick the first choice.\n",
    "    \n",
    "    Args:\n",
    "        item (TrismikItem): Test item to process.\n",
    "        \n",
    "    Returns:\n",
    "        Any: Response to the test item (depends on item type).\n",
    "    \"\"\"\n",
    "    if isinstance(item, TrismikMultipleChoiceTextItem):\n",
    "        # For multiple choice items, we need to return a choice id\n",
    "        # Here we just pick the first choice as a mock response\n",
    "        # In a real scenario, you would:\n",
    "        # 1. Use item.question as input to your model\n",
    "        # 2. Present item.choices as options\n",
    "        # 3. Post-process the model output to ensure it's a valid choice id\n",
    "        return item.choices[0].id\n",
    "    else:\n",
    "        raise RuntimeError(\"Encountered unknown item type\")\n",
    "\n",
    "def print_score(score: AdaptiveTestScore) -> None:\n",
    "    \"\"\"Print adaptive test score with theta and standard error.\"\"\"\n",
    "    print(\"\\nAdaptive Test Score:\")\n",
    "    print(f\"Final theta: {score.theta:.4f}\")\n",
    "    print(f\"Final standard error: {score.std_error:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up run metadata for our mock test\n",
    "mock_metadata = TrismikRunMetadata(\n",
    "    model_metadata=TrismikRunMetadata.ModelMetadata(\n",
    "        name=\"mock-model\",\n",
    "        parameters=\"N/A\",\n",
    "        provider=\"Demo\",\n",
    "    ),\n",
    "    test_configuration={\n",
    "        \"task_name\": dataset,\n",
    "        \"response_format\": \"Multiple-choice\",\n",
    "        \"description\": \"Mock inference demonstration\",\n",
    "    },\n",
    "    inference_setup={\n",
    "        \"strategy\": \"first_choice_selection\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run the mock inference run\n",
    "print(\"Running mock inference run...\")\n",
    "mock_runner = AdaptiveTest(mock_inference)\n",
    "mock_results = mock_runner.run(\n",
    "    dataset,\n",
    "    PROJECT_ID,\n",
    "    EXPERIMENT,\n",
    "    run_metadata=mock_metadata,\n",
    "    return_dict=False,\n",
    ")\n",
    "\n",
    "print(f\"Run {mock_results.run_id} completed.\")\n",
    "if mock_results.score is not None:\n",
    "    print_score(mock_results.score)\n",
    "else:\n",
    "    print(\"No score available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Local Model Inference with Transformers\n",
    "\n",
    "This section demonstrates how to run adaptive testing with a local Hugging Face transformers model. We'll use a lightweight model that can run efficiently on most hardware."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "import transformers\n",
    "\n",
    "# Set up the model pipeline\n",
    "# Using Phi-4-mini-instruct as it's relatively lightweight\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-4-mini-instruct\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def transformers_inference(\n",
    "    pipeline: transformers.pipeline, item: TrismikItem, max_retries: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference on an item using a Hugging Face model.\n",
    "    \n",
    "    Args:\n",
    "        pipeline (transformers.pipeline): Hugging Face pipeline.\n",
    "        item (TrismikItem): Item to run inference on.\n",
    "        max_retries (int): Maximum number of retries.\n",
    "    \"\"\"\n",
    "    assert isinstance(item, TrismikMultipleChoiceTextItem)\n",
    "    \n",
    "    # Construct the prompt from the question and choices\n",
    "    prompt = f\"{item.question}\\nOptions:\\n\" + \"\\n\".join(\n",
    "        [f\"- {choice.id}: {choice.text}\" for choice in item.choices]\n",
    "    )\n",
    "    \n",
    "    # System message with strict instructions\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "Answer the question you are given using only a single letter \\\n",
    "(for example, 'A'). \\\n",
    "Do not use punctuation. \\\n",
    "Do not show your reasoning. \\\n",
    "Do not provide any explanation. \\\n",
    "Follow the instructions exactly and \\\n",
    "always answer using a single uppercase letter.\n",
    "\n",
    "For example, if the question is \"What is the capital of France?\" and the \\\n",
    "choices are \"A. Paris\", \"B. London\", \"C. Rome\", \"D. Madrid\",\n",
    "- the answer should be \"A\"\n",
    "- the answer should NOT be \"Paris\" or \"A. Paris\" or \"A: Paris\"\n",
    "\n",
    "Please adhere strictly to the instructions.\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    final_answer = None\n",
    "    tries = 0\n",
    "    valid_ids = [choice.id for choice in item.choices]\n",
    "    \n",
    "    while final_answer is None and tries < max_retries:\n",
    "        outputs = pipeline(messages, **generation_args)\n",
    "        answer = outputs[0][\"generated_text\"].strip()\n",
    "        \n",
    "        # Post-process to extract just the letter if needed\n",
    "        if len(answer) != 1:\n",
    "            match = re.match(r\"^([A-Z]): .+\", answer)\n",
    "            if match:\n",
    "                answer = match.group(1)\n",
    "        \n",
    "        if answer in valid_ids:\n",
    "            final_answer = answer\n",
    "        else:\n",
    "            tries += 1\n",
    "    \n",
    "    if final_answer is None:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to run inference on question {item.question}, \"\n",
    "            f\"{item.choices}; the last model response was {answer}.\"\n",
    "        )\n",
    "    \n",
    "    return final_answer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up metadata for the transformers model\n",
    "transformers_metadata = TrismikRunMetadata(\n",
    "    model_metadata=TrismikRunMetadata.ModelMetadata(\n",
    "        name=\"microsoft/Phi-4-mini-instruct\",\n",
    "        parameters=\"14B\",\n",
    "        provider=\"Microsoft\",\n",
    "    ),\n",
    "    test_configuration={\n",
    "        \"task_name\": dataset,\n",
    "        \"response_format\": \"Multiple-choice\",\n",
    "    },\n",
    "    inference_setup={\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run with the transformers model\n",
    "print(\"Running transformers model...\")\n",
    "transformers_runner = AdaptiveTest(lambda item: transformers_inference(pipeline, item))\n",
    "transformers_results = transformers_runner.run(\n",
    "    dataset,\n",
    "    PROJECT_ID,\n",
    "    EXPERIMENT,\n",
    "    run_metadata=transformers_metadata,\n",
    "    return_dict=False,\n",
    ")\n",
    "\n",
    "print(f\"Run {transformers_results.run_id} completed.\")\n",
    "if transformers_results.score is not None:\n",
    "    print_score(transformers_results.score)\n",
    "else:\n",
    "    print(\"No score available.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: API-based Inference with OpenAI (Async)\n",
    "\n",
    "This section demonstrates how to use the Trismik SDK with OpenAI's API using asynchronous methods for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Initialize the async OpenAI client\n",
    "openai_client = AsyncOpenAI()\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "async def openai_inference_async(\n",
    "    client: AsyncOpenAI, item: TrismikItem, max_retries: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference on an item using the OpenAI API asynchronously.\n",
    "    \n",
    "    Args:\n",
    "        client (AsyncOpenAI): Async OpenAI client.\n",
    "        item (TrismikItem): Item to run inference on.\n",
    "        max_retries (int): Maximum number of retries.\n",
    "    \"\"\"\n",
    "    assert isinstance(item, TrismikMultipleChoiceTextItem)\n",
    "    \n",
    "    # Construct the prompt from the question and choices\n",
    "    prompt = f\"{item.question}\\nOptions:\\n\" + \"\\n\".join(\n",
    "        [f\"- {choice.id}: {choice.text}\" for choice in item.choices]\n",
    "    )\n",
    "    \n",
    "    # System message with strict instructions\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"\"\"\n",
    "Answer the question you are given using only a single letter \\\n",
    "(for example, 'A'). \\\n",
    "Do not use punctuation. \\\n",
    "Do not show your reasoning. \\\n",
    "Do not provide any explanation. \\\n",
    "Follow the instructions exactly and \\\n",
    "always answer using a single uppercase letter.\n",
    "\n",
    "For example, if the question is \"What is the capital of France?\" and the \\\n",
    "choices are \"A. Paris\", \"B. London\", \"C. Rome\", \"D. Madrid\",\n",
    "- the answer should be \"A\"\n",
    "- the answer should NOT be \"Paris\" or \"A. Paris\" or \"A: Paris\"\n",
    "\n",
    "Please adhere strictly to the instructions.\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    final_answer = None\n",
    "    tries = 0\n",
    "    valid_ids = [choice.id for choice in item.choices]\n",
    "    \n",
    "    while final_answer is None and tries < max_retries:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=10,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        if answer in valid_ids:\n",
    "            final_answer = answer\n",
    "        else:\n",
    "            tries += 1\n",
    "    \n",
    "    if final_answer is None:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to run inference on question {item.question}, \"\n",
    "            f\"{item.choices}; the last model response was {answer}.\"\n",
    "        )\n",
    "    \n",
    "    return final_answer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up metadata for the OpenAI model\n",
    "openai_metadata = TrismikRunMetadata(\n",
    "    model_metadata=TrismikRunMetadata.ModelMetadata(\n",
    "        name=model_name,\n",
    "        provider=\"OpenAI\",\n",
    "    ),\n",
    "    test_configuration={\n",
    "        \"task_name\": dataset,\n",
    "        \"response_format\": \"Multiple-choice\",\n",
    "    },\n",
    "    inference_setup={\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run the async test with OpenAI\n",
    "async def run_openai_test():\n",
    "    print(\"Running OpenAI async test...\")\n",
    "    \n",
    "    # Create an async inference function that captures the client\n",
    "    async def inference_wrapper(item: TrismikItem) -> str:\n",
    "        return await openai_inference_async(openai_client, item)\n",
    "    \n",
    "    openai_runner = AdaptiveTest(inference_wrapper)\n",
    "    openai_results = await openai_runner.run_async(\n",
    "        dataset,\n",
    "        PROJECT_ID,\n",
    "        EXPERIMENT,\n",
    "        run_metadata=openai_metadata,\n",
    "        return_dict=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"Run {openai_results.run_id} completed.\")\n",
    "    if openai_results.score is not None:\n",
    "        print_score(openai_results.score)\n",
    "    else:\n",
    "        print(\"No score available.\")\n",
    "    \n",
    "    return openai_results\n",
    "\n",
    "# Run the async test\n",
    "openai_results = await run_openai_test()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated three different approaches to using the Trismik adaptive testing framework:\n",
    "\n",
    "1. **Mock Inference**: A simple demonstration showing the basic structure and API usage\n",
    "2. **Local Model Inference**: Using Hugging Face transformers for local model evaluation\n",
    "3. **API-based Inference**: Using OpenAI's API with async methods for cloud-based evaluation\n",
    "\n",
    "Each approach shows how to:\n",
    "- Set up run metadata to track your model and test configuration\n",
    "- Implement inference functions that work with Trismik's test items\n",
    "- Run adaptive tests and interpret the results\n",
    "\n",
    "The key metrics to focus on are:\n",
    "- **Theta (Î¸)**: The primary score measuring model ability on the dataset\n",
    "- **Standard Error**: The uncertainty in the theta estimate (lower is better)\n",
    "\n",
    "Adaptive testing allows you to efficiently evaluate your models with significantly fewer test items while maintaining statistical rigor, making it ideal for iterative model development and evaluation workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
